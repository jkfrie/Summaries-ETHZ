\section{Design of Linear Discriminant Functions}
The problem of statistical decisions: n objects should be grouped in the classes 1,..,k and a doubt class D and a outlier class O.

\paragraph{Classification Error:}
The indicator function $\mathbb{I}_{\{\hat{c}(x)\!= y\}}$ counts errors.\\
\textbf{Expected errors} are also called the \textbf{expected risk}

\paragraph{Loss function:}
\begin{itemize}
    \item[-]Weighted mistakes: when classification errors are not equally costly or if we have class imbalance.
    \item[-]Loss function $L(y,z)$: denotes the loss for decision z if class y is correct.
\end{itemize}

Different Loss functions
\begin{itemize}
    \item 0-1 Loss: all classes are treated the same. We assume constant costs for missclassifications (and loss = d if int doubt). Conditional risk function of the classifier is the expected loss of class y (probability of misclassif. + probability of doubt)
    
    \item Surrogate Loss: 0-1 loss is non-convex $\rightarrow$ use exponential loss, logistic loss or hinge loss function.
    
\end{itemize}

\section{Linear Classifier}
simple and computationally efficient.\\
\\
Linear Discriminant Function:
\begin{equation}
    g(x) = w_0 + \sum_{j<=d}w_jx_j = (w_0,w)(1,x)^T =: a^Tx'
\end{equation}
Quadratic Discriminant Function 
\begin{equation}
    g(x) = w_0 + \sum_{j>=d}w_jx_j + \sum_{j<=d}\sum_{l>=d}w_{jl}x_jx_l
\end{equation}

\paragraph{Linear discriminant with nonlinear feature transformation}
Use a linear classifier in a high dimensional feature space, generated by a non-linear transformation of feature vectors.

\paragraph{Homogeneous Coordinates}
$x' = (1,x)^T$,  $a = (w_0,w)^T$\\
Simplifies the notation and the analysis substantially.

\paragraph{non unique solutions:} Introduce a margin b to classify data with a safe distance from decision boundary i.e. $a^Tx_i >= $\\
\textbf{Regularization of classifier by margin b!}

\paragraph{Algorithms for learning the Weight Vector:}
\begin{itemize}
    \item[-]Gradient Descent (GD): Go downwards towards steepest gradient. Takes as parameter the learning rate (stepsize)
    \item[-]Newtons Algorithm: Choose a(k+1) optimally to minimize second order Taylor expansion of loss function J(a(k+1)).
\end{itemize}

\paragraph{Perceptron Loss:} 1-0 Loss is non-convex use $-a^Tx_i$ instead
and perform Perceptron Algorithm (slides)\\
\\
Novikov Theroem: If the training samples are linearly seperable, then the sequence of the perceptron algorithm will terminate at a solution vector.

\subsection{Bayes Classifiers}
What decision functions do we use in bayes classification?

\paragraph{Bayes Optimal Classifier:}
The classification rule which minimizes the total risk for 0-1 loss is the maximum posterior of y (MAP) i.e. $max_y p(y|x)$ (if its bigger than doubt d)

\paragraph{Outliers}
Modeling by an outlier class $\pi_O$ with $p_O(x)$. \textbf{Novelty Detection:} Classify a measurement as an outlier if $\pi_Op_O(x) >= threshold$

\subsection{Fisher's Linear Discriminant Analysis: }
Project high-dimensional data of two classes to a one-dimensional linear subspace such that the classes are optimally separated.


